{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Prem AI and DSPy for Text to SQL Generation\n",
    "\n",
    "In this cookbook, we will be exploring how we can use PremAI SDK and DSPy to generate SQL from text. In this tutorial we are going to:\n",
    "\n",
    "1. Write simple prompts using `dspy.Signature` to give instruction to the LLM. \n",
    "2. Use `dspy.Module` to write a simple Text2SQL pipeline. \n",
    "3. Use `dspy.teleprompt` to automatically optimize the prompt for better result\n",
    "4. Use `dspy.Evaluate` to evaluate the results. \n",
    "\n",
    "### Objective\n",
    "\n",
    "The objective of this tutorial is to introduce you to DSPy and how to use it with Prem SDK. Finally in the later part of the tutorial we are also going to show you, how you can use Prem Platform to debug and see what DSPy has been optimizing and how it can be improved further. \n",
    "\n",
    "Before getting started, we install and import all our required packages.\n",
    "\n",
    "**NOTE:**\n",
    "\n",
    "For those who are not familiar with DSPy, DSPy is a LLM orchestration tool whose API is very similar to [PyTorch](https://pytorch.org/). The main focus of DSPy is to help developers write clean and modular code rather than writing very big prompts. You can learn more about it in their [documentation](https://dspy-docs.vercel.app/).\n",
    "\n",
    "Before getting started, we create a new virtual environment and install all our required packages from this [requirements.txt](/text-2-sql/requirements.txt) file. Once done let's import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anindyadeepsannigrahi/workspace/Prem/cookbook/text-2-sql/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dspy\n",
    "from dspy import PremAI\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.datasets import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define some constants and instantiate DSPy PremAI LM. We will be using CodeLLama as our base LLM to generate SQL Queries. Here is how our launchpad looks like\n",
    "\n",
    "![image](assets/text2sql_launchpad.png)\n",
    "\n",
    "If you are not familier with setting up a project on Prem, we recommend to take a quick look on [this guide](https://docs.premai.io/introduction). It is super intuitive and you can even get started for free. \n",
    "\n",
    "Prem AI offers a variety of models (see the [list](https://docs.premai.io/get-started/supported-models) here), so you can experiment with all the models. \n",
    "\n",
    "We did some initial experiments in [Prem Playground](https://app.premai.io/s/97e27016-3265-42e9-8358-68340a4d3ed7) and what we saw was CodeLlama was performing consistently better than other models like Claude 3, GPT-4o and Mistral. So for this cookbook experiment we are using Code Llama Instruct 70B by Meta AI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please change the API KEY and Project ID, This will not be valid \n",
    "# in your case. \n",
    "\n",
    "PREMAI_API_KEY = \"G91lPIK3XDX7ohwxu6EIlmRDiHQnmO7SMn\"\n",
    "PROJECT_ID = \"4071\"\n",
    "generation_kwargs = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"max_tokens\": 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the following system prompt.\n",
    "\n",
    "```markdown\n",
    "You are an expert in SQL. You can understand and write complex SQL queries. You will be given some plain text as a questions and you are required to generate SQL query from that. Do not generate anything else. \n",
    "```\n",
    "\n",
    "Additionally we set the temperature to be 0.1 and max_tokens to 500. You can copy these configurations to your experiment to reproduce or get started with your own implementation.\n",
    "\n",
    "Now, we instantiate our `lm` object and test it before moving forward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"SELECT 'hello' AS message;\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = PremAI(\n",
    "    project_id=PROJECT_ID,\n",
    "    api_key=PREMAI_API_KEY,\n",
    "    **generation_kwargs\n",
    ")\n",
    "dspy.configure(lm=lm)\n",
    "\n",
    "lm(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Datasets\n",
    "\n",
    "We start by loading a dataset. We are going to use the [gretelai/synthetic_text_to_sql](https://huggingface.co/datasets/gretelai/synthetic_text_to_sql) dataset for our example. We are also going to split the dataset into validation and test splits. The code below shows how we load and split the dataset using DSPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader()\n",
    "\n",
    "# Load the dataset from huggingface\n",
    "\n",
    "trainset = data_loader.from_huggingface(\n",
    "    dataset_name=\"gretelai/synthetic_text_to_sql\",\n",
    "    fields=(\"sql_prompt\", \"sql_context\", \"sql\"),\n",
    "    input_keys=(\"sql_prompt\", \"sql_context\"),\n",
    "    split=\"train\"\n",
    ")\n",
    "\n",
    "testset = data_loader.from_huggingface(\n",
    "    dataset_name=\"gretelai/synthetic_text_to_sql\", \n",
    "    fields=(\"sql_prompt\", \"sql_context\", \"sql\"), \n",
    "    input_keys=(\"sql_prompt\", \"sql_context\"), \n",
    "    split=\"test\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sample a very small amount of data because we are not going to do any kind of fine-tuning here. DSPy will optimize our prompt by tweaking it in several ways (for example, adding optimal few-shot examples). However, DSPy also provides options for fine-tuning the weights as well, but that is out of the scope of this tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = data_loader.sample(dataset=trainset, n=100)\n",
    "testset = data_loader.sample(dataset=testset, n=75)\n",
    "\n",
    "_trainval = data_loader.train_test_split(\n",
    "    dataset=trainset, \n",
    "    test_size=0.25, \n",
    "    random_state=1399\n",
    ")\n",
    "\n",
    "trainset, valset = _trainval[\"train\"], _trainval[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take one sample which will help us to do initial checks for our implementations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SQL_PROMPT:\n",
      "\n",
      "Find the top 3 menu items with the highest profit margin in each restaurant.\n",
      "\n",
      "SQL_CONTEXT:\n",
      "\n",
      "CREATE TABLE MenuItems (restaurant_id INT, menu_item_id INT, cost DECIMAL(10,2), price DECIMAL(10,2)); INSERT INTO MenuItems (restaurant_id, menu_item_id, cost, price) VALUES (1, 101, 5.00, 12.00), (1, 102, 6.00, 15.00), (1, 103, 4.50, 11.00), (2, 101, 4.00, 10.00), (2, 102, 5.50, 14.00);\n",
      "\n",
      "SQL:\n",
      "\n",
      "SELECT restaurant_id, menu_item_id, cost, price, (price - cost) as profit_margin FROM menuitems WHERE profit_margin IN (SELECT MAX(profit_margin) FROM menuitems GROUP BY restaurant_id, FLOOR((ROW_NUMBER() OVER (PARTITION BY restaurant_id ORDER BY profit_margin DESC)) / 3)) ORDER BY restaurant_id, profit_margin DESC;\n"
     ]
    }
   ],
   "source": [
    "sample = data_loader.sample(dataset=trainset, n=1)[0]\n",
    "\n",
    "for k, v in sample.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a DSPy Signature\n",
    "\n",
    "Most LLM orchestration frameworks, like langchain and llama-index, tell you to prompt the language models explicitly. Eventually, we need to tweak or optimize the prompt to seek better results. However, writing and managing big prompts can be super messy.\n",
    "\n",
    "The whole point of DSPy is to shift the whole prompting process to a more programmatic paradigm. Signature in DSPy lets you specify how the language model's input and output behaviour should be. \n",
    "\n",
    "**NOTE**\n",
    "> The Field names inside a Signature have a semantic significance. This means each field that represents a role (for example: `question` or `answer` or `sql_query`) defines that prompt variable and what the variable is about. More explained below in an example. \n",
    "\n",
    "For this example, we will try to understand class-based DsPy Signatures. You can learn more about Signatures in the [official documentation](https://dspy-docs.vercel.app/docs/building-blocks/signatures). \n",
    "\n",
    "#### Class-based DsPY Signatures \n",
    "\n",
    "You define a class in class-based signatures, where:\n",
    "\n",
    "- The class docstring is used to express the nature of the overall task. \n",
    "\n",
    "- Provide different input and output variables using `InputField` and `OutputField`, which also describe the nature of those variables. \n",
    "\n",
    "Here is a simple example of how we define a simple class-based DsPY Signatures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "    sentiment='sandness'\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Emotion(dspy.Signature):\n",
    "    # Define your overall task descripition here \n",
    "    \"\"\"Classify emotion among sandness, joy, love, anger, fear, surprise.\"\"\"\n",
    "\n",
    "    sentence = dspy.InputField(\n",
    "        desc=\"A sentence which needs to be classified\"\n",
    "    )\n",
    "\n",
    "    sentiment = dspy.OutputField(\n",
    "        desc=\"classify in either of one class: sandness / joy / love / anger / fear /surprise. Do not write anything else, just the class. Sentiment:\"\n",
    "    )\n",
    "\n",
    "classify = dspy.Predict(Emotion)\n",
    "classify(sentence=\"The day is super gloomy today ughhh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To inspect how the actual prompt was passed, you can check using `lm.inspect` method. Here is how it looks like for the `Emotion` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Classify emotion among sandness, joy, love, anger, fear, surprise.\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Sentence: A sentence which needs to be classified\n",
      "Sentiment: classify in either of one class: sandness / joy / love / anger / fear /surprise. Do not write anything else, just the class. Sentiment:\n",
      "\n",
      "---\n",
      "\n",
      "Sentence: The day is super gloomy today ughhh\n",
      "Sentiment:\u001b[32msandness\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\nClassify emotion among sandness, joy, love, anger, fear, surprise.\\n\\n---\\n\\nFollow the following format.\\n\\nSentence: A sentence which needs to be classified\\nSentiment: classify in either of one class: sandness / joy / love / anger / fear /surprise. Do not write anything else, just the class. Sentiment:\\n\\n---\\n\\nSentence: The day is super gloomy today ughhh\\nSentiment:\\x1b[32msandness\\x1b[0m\\n\\n\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we make a signature for our SQL Generator. In this signature, we will \n",
    "\n",
    "1. Define the overall task in the class docstring.\n",
    "2. Provide input and output prompt variables. \n",
    "3. Provide the description of Input and Output variables to tell what is expected here. \n",
    "\n",
    "You can also think of making a prompt very similar to the one shown below but in a more programable way:\n",
    "\n",
    "```python\n",
    "prompt = \"\"\"\n",
    "Transform a natural language query into an SQL query.\n",
    "Do not output anything other than SQL Query. You will be given the \n",
    "sql_prompt which will tell what you need to do and sql_context\n",
    "which will give some additional context to generate the right SQL. Here are the inputs:\n",
    "\n",
    "Natural language query: {sql_prompt}\n",
    "The context of the query: {sql_context}\n",
    "\n",
    "Write the SQL here: \n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2SQLSignature(dspy.Signature):\n",
    "    \"\"\"Transform a natural language query into a SQL query.\n",
    "    You will be given the sql_prompt which will tell what you need to do \n",
    "    and a sql_context which will give some additional context to generate the right SQL.\n",
    "    Only generate the SQL query nothing else. You should give one correct answer.\n",
    "    starting and ending with ```\n",
    "    \"\"\"\n",
    "\n",
    "    sql_prompt = dspy.InputField(desc=\"Natural language query\")\n",
    "    sql_context = dspy.InputField(desc=\"Context for the query\")\n",
    "    sql = dspy.OutputField(desc=\"SQL Query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's generate a single sample with the signature we just wrote. Please note that the signature is just a blueprint of what we want to achieve. You are not required to tweak the prompt anymore. It is the job of DSPy to do optimization to make the best prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SQL:\n",
      "\n",
      "```\n",
      "WITH profit_margin AS (\n",
      "  SELECT restaurant_id, menu_item_id, (price - cost) / price AS margin\n",
      "  FROM MenuItems\n",
      ")\n",
      "SELECT restaurant_id, menu_item_id, margin\n",
      "FROM (\n",
      "  SELECT restaurant_id, menu_item_id, margin,\n",
      "         ROW_NUMBER() OVER (PARTITION BY restaurant_id ORDER BY margin DESC) AS row_num\n",
      "  FROM profit_margin\n",
      ") AS subquery\n",
      "WHERE row_num <= 3;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "generate_sql_from_query = dspy.Predict(signature=Text2SQLSignature)\n",
    "result = generate_sql_from_query(\n",
    "    sql_prompt=sample[\"sql_prompt\"],\n",
    "    sql_context=sample[\"sql_context\"]\n",
    ")\n",
    "\n",
    "for k, v in result.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DSPy Module\n",
    "\n",
    "Now we know how Signatures work in DSPy. Multiple signatures or a prompting technique could work as a function. You can compose those techniques into a single module or a program. \n",
    "\n",
    "If you come from a deep learning background, then you have heard about `torch.nn.Module`, which helps to compose multiple layers to a single program. \n",
    "\n",
    "Similarly, in DSPy, multiple modules can be composed into bigger modules (programs). Let's create a simple module for our Text2SQL generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2SQLProgram(dspy.Module):\n",
    "    def __init__(self, signature: dspy.Signature):\n",
    "        super().__init__()\n",
    "        self.program = dspy.Predict(signature=signature)\n",
    "\n",
    "    def forward(self, sql_prompt, sql_context):\n",
    "        return self.program(\n",
    "            sql_prompt=sql_prompt,\n",
    "            sql_context=sql_context\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you might wonder why we are doing the same thing but now wrapping inside another class? The reason is that you might have multiple such signatures. Each signature might have different purposes. You can chain each of them individually to come up with a single output. However, that is out of the scope of the current tutorial. Now, let's run this module for a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SQL:\n",
      "\n",
      "```\n",
      "WITH profit_margin AS (\n",
      "  SELECT restaurant_id, menu_item_id, (price - cost) / price AS margin\n",
      "  FROM MenuItems\n",
      ")\n",
      "SELECT restaurant_id, menu_item_id, margin\n",
      "FROM (\n",
      "  SELECT restaurant_id, menu_item_id, margin,\n",
      "         ROW_NUMBER() OVER (PARTITION BY restaurant_id ORDER BY margin DESC) AS row_num\n",
      "  FROM profit_margin\n",
      ") AS subquery\n",
      "WHERE row_num <= 3;\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "text2sql = Text2SQLProgram(signature=Text2SQLSignature)\n",
    "result = text2sql(\n",
    "    sql_prompt=sample[\"sql_prompt\"],\n",
    "    sql_context=sample[\"sql_context\"]\n",
    ")\n",
    "\n",
    "for k, v in result.items():\n",
    "    print(f\"\\n{k.upper()}:\\n\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now that things are working, let's make a simple metrics in which we can quantify how well the SQL queries are being generated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics in DSPy\n",
    "\n",
    "A metric is a function that quantifies in some way how a ground truth is related to the prediction and how good the predicted output is. A simple example is accuracy. So, in our case, a very simple metric could be a direct string match on whether our predicted SQL string is identical with the actual SQL string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import sqlparse\n",
    "\n",
    "def normalise_sql_string(sql_string):\n",
    "    normalized = re.sub(r'```', '', sql_string).strip()\n",
    "    return re.sub(r'\\s+', ' ', normalized)\n",
    "\n",
    "def compare_sqls(ground_truth, prediction, trace=None):\n",
    "    ground_truth = ground_truth.sql\n",
    "    prediction = prediction.sql\n",
    "    \n",
    "    ground_truth = normalise_sql_string(sql_string=ground_truth)\n",
    "    prediction = normalise_sql_string(sql_string=prediction)\n",
    "\n",
    "    ground_truth_parsed = sqlparse.format(\n",
    "        ground_truth,\n",
    "        reindent=True,\n",
    "        keyword_case=\"upper\"\n",
    "    ).strip()\n",
    "\n",
    "    prediction_parsed = sqlparse.format(\n",
    "        prediction,\n",
    "        reindent=True,\n",
    "        keyword_case=\"upper\"\n",
    "    ).strip()\n",
    "\n",
    "    return ground_truth_parsed == prediction_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [01:16<00:00,  3.04s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "scores = []\n",
    "\n",
    "for x in tqdm(valset, total=len(valset)):\n",
    "    prediction = text2sql(\n",
    "        sql_prompt=x.sql_prompt, sql_context=x.sql_context\n",
    "    )\n",
    "    ground_truth = x\n",
    "    score = compare_sqls(ground_truth=ground_truth, prediction=prediction)\n",
    "    scores.append(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing a DSPy program\n",
    "\n",
    "An optimizer in DSPy optimizes the overall prompt workflow by tuning the prompt and/or the LM weights to maximize the target metrics, such as accuracy. \n",
    "\n",
    "Optimizers take three things in the input:\n",
    "\n",
    "1. **The DSPy Program**: This may be a single module (e.g., dspy. Predict) or a complex multi-module program. We have already defined this in the above cells.\n",
    "\n",
    "2. **Metric Function:** This is the function that evaluates the program and assigns a score in the end. We have already defined this in the above cell.\n",
    "\n",
    "3. **Few training inputs:** This may be very small (i.e., only 5 or 10 examples) and incomplete (only inputs to your program, without any labels).\n",
    "\n",
    "You can learn more about DSPy Optimizers in their [official documentation](https://dspy-docs.vercel.app/docs/building-blocks/optimizers). For this program, we are **NOT** optimizing any weights; hence we are not fine-tuning here. \n",
    "\n",
    "To keep things simple, we use the `LabeledFewShot` optimizer. It simply constructs a few shot examples (which we call as demos) from provided labelled input and output data points. Let's see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dspy.teleprompt import LabeledFewShot\n",
    "\n",
    "# k = number of few shot examples\n",
    "optimizer = LabeledFewShot(k=4)\n",
    "\n",
    "optimized_text2sql = optimizer.compile(\n",
    "    student=text2sql,\n",
    "    trainset=trainset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, finally we use the `dspy.Evaluate` to evaluate our overall system. Here is how we do it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 11 / 25  (44.0): 100%|██████████| 25/25 [00:22<00:00,  1.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate = Evaluate(\n",
    "    devset=valset,\n",
    "    metric=compare_sqls,\n",
    "    num_threads=3,\n",
    "    display_progress=True,\n",
    "    display_table=0\n",
    ")\n",
    "\n",
    "evaluate(optimized_text2sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several more types of optimizers, and all of them follow the same flow. So you can simply plug in and out each of them to test which works best. You can learn more about different DSPy optimizers [here](https://dspy-docs.vercel.app/docs/building-blocks/optimizers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this tutorial, we cooked a straightforward example on how we can generate SQL from Text. 44 % is a fair accuracy for starting out. In this example, we used CodeLLama. However we can use different models and see which works best for us. \n",
    "\n",
    "Additionally when you use Prem, you can actually see all the traces and runs of the model being captured in the [traces section](https://docs.premai.io/get-started/monitoring). Inside Traces, you can monitor each LLM run and see how DSPy optimized your prompt. Here is an example of our case:\n",
    "\n",
    "![](../assets/text2sql_traces.png)\n",
    "\n",
    "From the above picture, as you can see, DSPy added some in-context examples to optimize the initial prompt. You can similarly do this for different settings (like using a different LLM or using a different Optimizer and all of those will be captured here)\n",
    "\n",
    "\n",
    "Additionally, if you are using [Business plans](https://app.premai.io/users/organization/billing/), you can use these traces to further fine-tune your model to work even better. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
